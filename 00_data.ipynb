{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> A simple API for importing and preparing data for use. Mostly manipulates numpy arrays to generate profiles and sections\n",
    "> as well as plane levelling, noise removal and waviness removal/separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#note\n",
    "#Generate a gausspulse for testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be treating 2D arrays as rasters. Basically load any .csv, .txt or other file into a numpy  array as you would normally. Each entry should be the height data for it's respective pixel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the simplest method, a text file containing an (M,N) array compatible with Numpy.\n",
    "If you'd like to try your own data, simply change the file below and the loading function (e.g. if you have a .csv just change the delimeter in the np.loadtxt() call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'BYGS008_top_segment_500samp_10cm_interp089.txt'\n",
    "image = np.loadtxt(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "image_45 = imutils.rotate(image, angle = 50)\n",
    "plt.imshow(image_45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be very useful to study how roughness parameters change with regards to their orientation. The following function helps produce a range of profiles rotating around the central point of the image or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gen_rot_prof(array, #2D array of height values\n",
    "                 deg       = 180, #Number of degrees to rotate through, i.e 180 gives full 360 rotation\n",
    "                 increment = 1 # deg/increment = number of evenly spaced profiles to calculate.  \n",
    "                ):\n",
    "    \n",
    "    ''' Generates an array of rotational profiles through to deg, in even increments of increment. \n",
    "    Uses OpenCV and Imutils to rotate the array around the center of the array/raster/image, extracts the middle row. \n",
    "    '''\n",
    "    if deg % increment != 0:\n",
    "        raise ValueError('Cannot sample evenly, deg % indent must = 0')\n",
    "        \n",
    "    profiles = np.zeros(shape = (deg//increment,array.shape[0]))\n",
    "    index    = 0\n",
    "    center   = array.shape[0]//2  #Center is returned as index to the right of center for even arrays\n",
    "    \n",
    "    for degree in range(0, deg, increment):\n",
    "        rot_array          = imutils.rotate(array, angle = degree)\n",
    "        profiles[index, :] = rot_array[center,:]\n",
    "        index             += 1\n",
    "    return profiles\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy likes the data in various forms for linear algebra, \n",
    "here is a helper to convert an (M,N) matrix into a (n,(X,Y,Z)) matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def image2xyz(im):\n",
    "    '''\n",
    "    Converts 2D (m,n) image/array to xyz coordinates. Used for plane levelling\n",
    "    '''\n",
    "    \n",
    "    m, n = im.shape\n",
    "    Y, X = np.mgrid[:m,:n]\n",
    "    xyz = np.column_stack((X.ravel(),Y.ravel(), im.ravel()))\n",
    "    \n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def xyz2image(xyz, # (n,3) shape array \n",
    "             ):\n",
    "    '''\n",
    "    Helper to convert back from xyz (n,3) arrays to (M,N) image/matrices\n",
    "    '''\n",
    "    return xyz[:,2].reshape(np.max(xyz[:,1]) + 1,\n",
    "                            np.max(xyz[:,0]) + 1)\n",
    "\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_xyz = image2xyz(image)\n",
    "im_xyz[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levelling and Form Removal\n",
    "\n",
    "In order to perform roughness calculations it is recommended to level the data and remove the underlying form.This produces a _S-F_ surface from the *primary *surface if we are using Standards terms. Because surfaces are always digitized and discretized in some way, the actual surface has to be modelled using some function. ISO software standards recommend using a Bicubic spline to remove the form. Because the function is an assumption, the user should choose their function based on their scientific knowledge of the surface and the goals of their research. Multiple functions can be tested and the results observed.  Here I provide a least-squares solution to the problem, computing the results in the same shape as the original image and subtract them. \n",
    "\n",
    "With the underlying form modeled, the funciton can be sampled from to generate a larger number of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_form(im, # 2D Numpy array or array like\n",
    "               degree = 3, # Polynomial degree to remove\n",
    "               return_form = False # Return the form/computed polynomial values instead of removing them from im\n",
    "               ):\n",
    "    '''\n",
    "    Remove the form of the raster by fitting a polynomial of specified degree and subtracting it. \n",
    "    '''\n",
    "    imagexyz = image2xyz(im)\n",
    "    imagexy  = imagexyz[:,:2]\n",
    "    imagez   = imagexyz[:,2]\n",
    "    \n",
    "    poly     = sklearn.preprocessing.PolynomialFeatures(degree=degree, include_bias = False) #No bias as it is introduced later\n",
    "    features = poly.fit_transform(imagexy)\n",
    "    \n",
    "    poly_reg_model = sklearn.linear_model.LinearRegression() #Polynomial Regression Model\n",
    "    poly_reg_model.fit(features, imagez)\n",
    "    \n",
    "    predictions    =  poly_reg_model.predict(features) #Get the fitted values\n",
    "    form = predictions.reshape(int(np.max(imagexyz[:,1])) + 1, #Reshape the predictions into the original image dimensions\n",
    "                               int(np.max(imagexyz[:,0])) + 1)\n",
    "    if return_form:\n",
    "        return form\n",
    "    else:\n",
    "        return im - form\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def plane_level(im, #Numpy array or array like\n",
    "                norm = True, #Normalize the data by subtracting the mean\n",
    "                return_form = False\n",
    "               ):\n",
    "    '''\n",
    "    Level an (m,n) array by computing the best fit plane and subtracting the results.\n",
    "    Thin wrapper around `remove_form` with degree = 1. \n",
    "    '''\n",
    "    if norm:\n",
    "        im = im - np.mean(im, axis = None)\n",
    "        \n",
    "    return remove_form(im = im, degree = 1, return_form = return_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n",
    "u = np.array([[1,0,-1]]*3)\n",
    "test_close(plane_level(w), np.zeros(w.shape))\n",
    "test_close(plane_level(u), np.zeros(u.shape))\n",
    "test_fail(plane_level, kwargs = dict(xyz=np.array([1])))\n",
    "test_fail(plane_level, kwargs = dict(xyz=np.array([[1,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(plane_level(image))\n",
    "ax.set_title('Levelled image')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(plane_level(image, return_form = True))\n",
    "ax.set_title('Levelling plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_f = remove_form(plane_level(image))\n",
    "image_form = remove_form(plane_level(image), return_form = True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(image_f)\n",
    "ax.set_title('Formless Image')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(image_form)\n",
    "ax.set_title('Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise and smoothing\n",
    "\n",
    "Similarly, it is recommended to remove noise and attenuate high frequency features. We achieve this through the use of a gaussian filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def smooth_image(array,         #Numpy array or array like\n",
    "                 sigma = None,  #Standard deviation for gaussian kernel Useful for determining the wavelength of the low pass filter.\n",
    "                 alpha = None,  #Used in gaussian weighting function, defaults to np.sqrt(np.log(2)/np.pi) \n",
    "                 cutoff = None, #Cutoff wavelength, defaults to 1\n",
    "                 axis  = None,  # Axis along which to apply filter \n",
    "                 **kwargs #Keyword arguments for modification of the gaussian_filter function\n",
    "                ):\n",
    "    '''\n",
    "    Removes high frequency/wavelength features ('noise') by applying a gaussian filter on the image. \n",
    "    Thin wrapper of scipy.ndimage.gaussian_filter.\n",
    "    \n",
    "    If all sigma,alpha,cutoff =  None, sigma defaults to (np.sqrt(np.log(2)/np.pi)) * cutoff\n",
    "    \n",
    "    If sigma is not none, sigma takes priority over any alpha or cutoff provided. \n",
    "    \n",
    "    Refer to ISO 11562:1997 for reasoning behind alpha and cutoff.\n",
    "    '''\n",
    "    if sigma is None:\n",
    "        if alpha is None: \n",
    "            alpha = np.sqrt(np.log(2)/np.pi)\n",
    "        if cutoff is None:\n",
    "            cutoff = 1\n",
    "        sigma = alpha * cutoff \n",
    "    \n",
    "    return ndimage.gaussian_filter(input = array, sigma = sigma, axes = axis, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_f_s = smooth_image(image_f,cutoff=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(image_f_s)\n",
    "ax.set_title('Sigma = 1')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(smooth_image(image_f,cutoff=10))\n",
    "ax.set_title('Sigma = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    " \n",
    " It can be useful to study subsections of surfaces. The following helpers assist with this process. Otherwise, normal manipulation of numpy arrays is always possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gen_sections(image, #2D array (or arraylike) of height values\n",
    "                how = 'square', #How to subdivide the array, options are: 'square', 'row', 'column'\n",
    "                number = 100, #Number of sections to produce\n",
    "                \n",
    "                ):\n",
    "    '''\n",
    "    Generates sections of the array/image, either in square, horizontal, or vertical sections.\n",
    "    Useful for studying the change of parameters over the surface.\n",
    "    Mostly wraps around np.hsplit and np.vsplit.\n",
    "    Note, if 'number' does not divide into the array evenly, the bottom/side remains will not be\n",
    "    included. \n",
    "    '''\n",
    "    if how not in ['square','row','column']:\n",
    "        raise ValueError('Invalid how, expected one of:')\n",
    "   \n",
    "    if how == 'square':\n",
    "        im_height, im_width = image.shape\n",
    "        length   = number**0.5\n",
    "        roww     = int(im_height//length)\n",
    "        colw     = int(im_width//length) \n",
    "\n",
    "        image = image[:(im_height - (im_height % roww)), :(im_width - (im_width % colw))] #Remove extra rows/columns\n",
    "        \n",
    "        #https://towardsdatascience.com/efficiently-splitting-an-image-into-tiles-in-python-using-numpy-d1bf0dd7b6f7\n",
    "        #reshape_formula = a_10000.reshape(int(im_height/tile),tile,int(im_width/tile),tile,channels)\n",
    "        image_reshaped = image.reshape(int(im_height/roww),roww,int(im_width/colw),colw)\n",
    "        \n",
    "        tiled_image = image_reshaped.swapaxes(1,2)\n",
    "        \n",
    "        sectioned_image = tiled_image.reshape(-1,roww,colw)\n",
    "        return sectioned_image\n",
    "    \n",
    "    if how == 'row':\n",
    "        return np.vsplit(image, number)\n",
    "    \n",
    "    if how == 'column':\n",
    "        return np.hsplit(image, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_10000 = np.arange(100*100).reshape(100,100)\n",
    "test_eq(gen_sections(a_10000)[0],a_10000[:10,:10])\n",
    "\n",
    "a_523 = np.arange(523*523).reshape(523,523)\n",
    "a_520 = np.arange(520*520).reshape(520,520)\n",
    "test_eq(gen_sections(a_523).shape,gen_sections(a_520).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sections = np.load('example_sections.npy')\n",
    "test_sections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sections =  gen_sections(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because we've applied all of our preprocessing steps to the original image. We can export it for use later.\n",
    "We should also save our profiles and sections. The sections should be in .npy format because they are 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('example.txt', image_f_s)\n",
    "np.savetxt('example_profiles.txt', gen_rot_prof(image_f_s))\n",
    "np.save('example_sections.npy', image_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can load them back in just to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = np.loadtxt('example_profiles.txt')\n",
    "plt.imshow(profiles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "> Various useful functions which are used elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_parameters(array, #Input array to be calculate parameters on\n",
    "                       parameter_list:list,  #List of parameters to calculate as strings\n",
    "                       valid_module  = None, #module to generate functions from, used to check user input, see rough.cli:rough\n",
    "                       to_df:bool    = False,#Return the parameters as a pandas dataframe, with columns set as the parameter names\n",
    "                       **kwargs              #Keyword arguments to modify behavior of parameter calls, usually to define sections = True or the axis. \n",
    "                      ):\n",
    "    '''\n",
    "    Computes a set of parameters for a given array, provide a list of parameters (as strings of their respective functions e.g. ['Ra','Rms']) and a module\n",
    "    to verify against (might require some module aliasing, see CLI notebook for example use). Returns a list of results or a dataframe.\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    #The following generates a {'func':func} dict from given list of parameters if the parameter is available in the module\n",
    "    valid_dict = {k: v for k, v in vars(valid_module).items() if callable(v) and k in valid_module.__all__}\n",
    "    for parameter in parameter_list:\n",
    "        result = valid_dict[parameter](array, **kwargs)\n",
    "        results.append(result)\n",
    "        \n",
    "    if to_df:\n",
    "        results_array = np.array(results)\n",
    "        if len(results_array.shape) == 1: results_array = np.expand_dims(results_array, axis=1) #Fix for when only 1 section is being calculated\n",
    "        return pd.DataFrame(data = results_array.T, columns = parameter_list)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def distance_matrix(shape: tuple, #Shape of array, used to calculate center if not given\n",
    "                    center: (int,int) = None, #Central point from which to calculate distances, if None, defaults to x//2, y//2\n",
    "                    sections = False, #If True, takes the first element of shape as the number of stack in image\n",
    "                   ):\n",
    "    '''\n",
    "    Returns a (m,n) matrix containing distance values from center coordinates.\n",
    "    \n",
    "    if Sections = True. Returns (x,m,n) where x is the number of input sections. \n",
    "    \n",
    "    '''\n",
    "    if sections:\n",
    "        n_stack = shape[0]\n",
    "        shape   = shape[1:]\n",
    "    if center is None:\n",
    "        center = (shape[0]//2,shape[1]//2)\n",
    "        \n",
    "    y_arr, x_arr = np.mgrid[0:shape[0],0:shape[1]]\n",
    "    #Pythagoras\n",
    "    return np.sqrt(((y_arr - center[0])**2) + ((x_arr - center[1]) ** 2))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a25 = np.arange(25).reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a25.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a25t = np.tile(a25,(5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(a25t[0],a25t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shape = (101,101)\n",
    "#eps = 1e-05\n",
    "test_eq(distance_matrix(test_shape), np.rot90(distance_matrix(test_shape)))\n",
    "test_eq(distance_matrix(test_shape), np.flipud(distance_matrix(test_shape)))\n",
    "test_eq(distance_matrix(test_shape), np.fliplr(distance_matrix(test_shape)))\n",
    "\n",
    "test_ne(distance_matrix(test_shape), np.zeros(test_shape))\n",
    "test_ne(distance_matrix(test_shape), np.ones(test_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalize(im, #Array or stack of array to normalize\n",
    "              axis = 1, #Axis along which to normalize\n",
    "              how = 'center', #normalization method: 'center', 'standardize', 'minmax'\n",
    "              feature_range = None, #Tuple containing the feature range for minmax\n",
    "             ):\n",
    "    '''\n",
    "    Normalize the input array along given axis. Typically used to 'center' rows/columns/areas in order to calculate parameters.\n",
    "    how can be:\n",
    "    - 'center': Subtract the mean from the array along the axis,\n",
    "    - 'l1'\n",
    "    - 'l2'\n",
    "    - 'standardize' : Subtract the mean and divide by the standard deviation along given axis\n",
    "    - 'minmax' : 'standardize' within 'feature_range'. See use in `Sal`\n",
    "    \n",
    "    Mostly a reimplementation of scalers from sklearn with explicit formulation. \n",
    "    '''\n",
    "    if how == 'center':\n",
    "        return im - np.mean(im,axis=axis,keepdims= True)\n",
    "    elif how == 'standardize':\n",
    "        return ((im - np.mean(im,axis=axis,keepdims=True)) / np.std(im,axis=axis,keepdims= True))\n",
    "    elif how == 'minmax':\n",
    "        if feature_range is None:\n",
    "            feature_range = (-1,1)\n",
    "            \n",
    "        im_std = (im - np.amin(im,axis=axis,keepdims=True)) / np.ptp(im,axis=axis,keepdims=True)\n",
    "        min_r,max_r = feature_range \n",
    "        return im_std * (max_r - min_r) + min_r\n",
    "    elif how == 'none':\n",
    "        return im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing normalize\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_400 =np.arange(-200,200).reshape(-1,1)\n",
    "a_rand = np.random.randint(-1000,1000,1000).reshape(-1,1)\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "\n",
    "test_eq((a_400 - np.mean(a_400,axis = 0,keepdims = True)), normalize(a_400, axis = 0, how = 'center'))\n",
    "test_eq((a_rand - np.mean(a_rand,axis = 0, keepdims = True)), normalize(a_rand,axis = 0, how = 'center'))\n",
    "\n",
    "test_close(scaler.fit_transform(a_400),  normalize(a_400,axis = 0, how = 'minmax'), eps = 1e-10)\n",
    "test_close(scaler.fit_transform(a_rand), normalize(a_rand,axis = 0, how = 'minmax'), eps = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_f_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_result = correlate(image_f_s, image_f_s,mode='same')\n",
    "plt.imshow(cor_result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate(np.ones((3,3)), np.ones((3,3)), mode = 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr = np.random.rand(501,501)\n",
    "rand_cor = correlate(rand_arr,rand_arr,mode='same')\n",
    "plt.imshow(rand_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.amax(rand_cor),np.amin(rand_cor),np.ptp(rand_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ncor = normalize(rand_cor,axis = None, how= 'minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.amax(rand_ncor),np.amin(rand_ncor),np.ptp(rand_ncor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rand_ncor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dist = distance_matrix(rand_ncor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_where = np.where(rand_ncor <= 0.2, rand_dist, np.NaN)\n",
    "np.nanmin(rand_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 20)\n",
    "y = np.sin(x)\n",
    "sin_wave = np.broadcast_to(y,(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sin_wave)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sin_wave.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_cor = correlate(sin_wave,sin_wave,mode='same')\n",
    "plt.imshow(sin_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncor_sin = normalize(sin_cor, axis = None, how = 'minmax')\n",
    "plt.imshow(ncor_sin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncor_sin_2 = np.where(ncor_sin <=0.2, ncor_sin, np.NaN)\n",
    "plt.imshow(ncor_sin_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sint_cor = correlate(sin_wave.T,sin_wave.T,mode='same')\n",
    "plt.imshow(sint_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_cor[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ptp(cor_result),np.max(cor_result),np.min(cor_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
